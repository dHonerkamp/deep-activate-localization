2021-05-17 08:16:35.158710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:root:Argument blacklist is deprecated. Please use denylist.
WARNING:root:Argument blacklist is deprecated. Please use denylist.
INFO:root:Importing iGibson (gibson2 module)
INFO:root:Assets path: /home/guttikon/activate-localization/igibson/gibson2/data/assets
INFO:root:Gibson Dataset path: /home/guttikon/activate-localization/igibson/gibson2/data/g_dataset
INFO:root:iG Dataset path: /home/guttikon/activate-localization/igibson/gibson2/data/ig_dataset
INFO:root:3D-FRONT Dataset path: /home/guttikon/activate-localization/igibson/gibson2/data/threedfront_dataset
INFO:root:CubiCasa5K Dataset path: /home/guttikon/activate-localization/igibson/gibson2/data/cubicasa_dataset
INFO:root:Example path: /home/guttikon/activate-localization/igibson/gibson2/examples
INFO:root:Example config path: /home/guttikon/activate-localization/igibson/gibson2/examples/configs
pybullet build time: May  2 2021 09:44:36
torch is not available, falling back to rendering to memory(instead of tensor)
/home/guttikon/.local/lib/python3.6/site-packages/absl/flags/_validators.py:356: UserWarning: Flag --config_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
==================================================
logtostderr False
alsologtostderr False
log_dir 
v 0
verbosity 0
logger_levels {}
stderrthreshold fatal
showprefixforinfo True
run_with_pdb False
pdb_post_mortem False
pdb False
run_with_profiling False
profile_file None
use_cprofile_for_profiling True
only_check_args False
op_conversion_fallback_to_while_loop True
runtime_oom_exit True
hbm_oom_exit True
test_random_seed 301
test_srcdir 
test_tmpdir /tmp/absl_testing
test_randomize_ordering_seed 
xml_output_file 
root_dir train_output
gin_file None
gin_param None
num_epochs 25
num_environment_steps 500000
collect_episodes_per_iteration 30
num_parallel_environments 1
num_parallel_environments_eval 1
replay_buffer_capacity 10001
learning_rate 0.0003
seed 100
use_tf_functions False
use_parallel_envs False
num_eval_episodes 10
eval_interval 500
eval_only False
eval_deterministic False
gpu_c 0
config_file ./configs/turtlebot_point_nav.yaml
model_ids None
model_ids_eval None
env_mode headless
action_timestep 0.1
physics_timestep 0.025
gpu_g 0
? False
help False
helpshort False
helpfull False
helpxml False
conv_1d_layer_params [(32, 8, 4), (64, 4, 2), (64, 3, 1)]
conv_2d_layer_params [(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 2)]
encoder_fc_layers [256]
actor_fc_layers [256]
value_fc_layers [256]
==================================================
2021-05-17 08:17:40.770801: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-05-17 08:17:40.776074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-05-17 08:17:40.860257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:17:40.860944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.90GiB deviceMemoryBandwidth: 447.48GiB/s
2021-05-17 08:17:40.860961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-17 08:17:46.831353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-05-17 08:17:48.969625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-05-17 08:17:50.167195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-05-17 08:17:56.448432: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-05-17 08:18:02.343583: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-05-17 08:18:04.178570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-05-17 08:18:06.637951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-05-17 08:18:06.638268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:06.640460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:06.642375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-17 08:18:06.643036: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-05-17 08:18:06.643486: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-05-17 08:18:06.643673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:06.644369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.90GiB deviceMemoryBandwidth: 447.48GiB/s
2021-05-17 08:18:06.644383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-17 08:18:06.644405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-05-17 08:18:06.644415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-05-17 08:18:06.644426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-05-17 08:18:06.644436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-05-17 08:18:06.644446: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-05-17 08:18:06.644456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-05-17 08:18:06.644466: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-05-17 08:18:06.644509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:06.645116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:06.645695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-17 08:18:06.645716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-05-17 08:18:30.404783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-05-17 08:18:30.405477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-05-17 08:18:30.405494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-05-17 08:18:30.405695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:30.406379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:30.407033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-17 08:18:30.407622: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2021-05-17 08:18:30.407650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11010 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)
I0517 08:18:31.693141 139875764696896 get_available_devices.py:31] Device 0 is available for rendering
I0517 08:18:31.796190 139875764696896 get_available_devices.py:31] Device 1 is available for rendering
I0517 08:18:31.815840 139875764696896 get_available_devices.py:33] Command '['/home/guttikon/activate-localization/igibson/gibson2/render/mesh_renderer/build/test_device', '2']' returned non-zero exit status 1.
I0517 08:18:31.816092 139875764696896 get_available_devices.py:34] Device 2 is not available for rendering
I0517 08:18:31.835813 139875764696896 get_available_devices.py:33] Command '['/home/guttikon/activate-localization/igibson/gibson2/render/mesh_renderer/build/test_device', '3']' returned non-zero exit status 1.
I0517 08:18:31.836056 139875764696896 get_available_devices.py:34] Device 3 is not available for rendering
I0517 08:18:32.083469 139875764696896 mesh_renderer_cpu.py:87] Using device 0 for rendering
W0517 08:18:32.691471 139875764696896 mesh_renderer_cpu.py:228] Environment texture not available, cannot use PBR.
******************PyBullet Logging Information:
PyBullet Logging Information******************
I0517 08:18:32.729770 139875764696896 indoor_scene.py:45] IndoorScene model: Rs
I0517 08:18:32.729924 139875764696896 gibson_indoor_scene.py:54] StaticIndoorScene scene: Rs
I0517 08:18:41.068964 139875764696896 indoor_scene.py:118] Loading traversable graph
I0517 08:18:41.142796 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/g_dataset/Rs/mesh_z_up.obj
/home/guttikon/activate-localization/igibson/gibson2/render/mesh_renderer/mesh_renderer_cpu.py:464: RuntimeWarning: divide by zero encountered in true_divide
  delta_uv1[:, 1] * delta_uv2[:, 0])
/home/guttikon/activate-localization/igibson/gibson2/render/mesh_renderer/mesh_renderer_cpu.py:466: RuntimeWarning: invalid value encountered in multiply
  delta_pos2 * delta_uv1[:, 1][:, None]) * r[:, None]
/home/guttikon/activate-localization/igibson/gibson2/render/mesh_renderer/mesh_renderer_cpu.py:468: RuntimeWarning: invalid value encountered in multiply
  delta_pos1 * delta_uv2[:, 0][:, None]) * r[:, None]
I0517 08:18:45.053649 139875764696896 robot_base.py:42] Loading robot model file: turtlebot/turtlebot.urdf
I0517 08:18:47.126858 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/kobuki_description/meshes/main_body.obj
I0517 08:18:47.301856 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/mjcf_primitives/cube.obj
I0517 08:18:47.312578 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/kobuki_description/meshes/wheel.obj
I0517 08:18:47.366210 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/mjcf_primitives/cube.obj
I0517 08:18:47.366779 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/mjcf_primitives/cube.obj
I0517 08:18:47.367227 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/stacks/hexagons/pole_bottom.obj
I0517 08:18:47.380797 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/stacks/hexagons/plate_bottom.obj
I0517 08:18:47.494391 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/stacks/hexagons/pole_middle.obj
I0517 08:18:47.506193 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/stacks/hexagons/plate_middle.obj
I0517 08:18:47.621200 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/stacks/hexagons/pole_top.obj
I0517 08:18:47.633398 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/stacks/hexagons/pole_kinect.obj
I0517 08:18:47.647935 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/stacks/hexagons/plate_top.obj
I0517 08:18:47.681524 139875764696896 mesh_renderer_cpu.py:335] Loading /home/guttikon/activate-localization/igibson/gibson2/data/assets/models/turtlebot/turtlebot_description/meshes/sensors/kinect.obj
=====> NavigateGibsonEnv initialized
observation_spec OrderedDict([('task_obs', BoundedTensorSpec(shape=(20,), dtype=tf.float32, name='observation/task_obs', minimum=array(-1000., dtype=float32), maximum=array(1000., dtype=float32))), ('rgb_obs', BoundedTensorSpec(shape=(56, 56, 3), dtype=tf.float32, name='observation/rgb_obs', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))])
action_spec BoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))
2021-05-17 08:18:49.430084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-05-17 08:20:37.579666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-05-17 08:20:54.275576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
W0517 08:20:54.701649 139875764696896 ppo_agent.py:329] Only tf.keras.optimizers.Optimiers are well supported, got a non-TF2 optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x7f3633098048>
I0517 08:20:54.970307 139875764696896 common.py:980] No checkpoint available at train_output/train
I0517 08:20:54.975001 139875764696896 common.py:980] No checkpoint available at train_output/train/policy
2021-05-17 08:20:55.091041: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-05-17 08:20:55.111327: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3199980000 Hz
I0517 08:26:22.508048 139875764696896 metric_utils.py:47]  
		 AverageReturn = -0.2500000298023224
		 AverageEpisodeLength = 300.0
I0517 08:26:22.508738 139875764696896 train_eval_clip_agent.py:340] Training starting .....
WARNING:tensorflow:5 out of the last 6023 calls to <function TFStepMetric._update_state at 0x7f363255cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
W0517 08:26:23.040619 139875764696896 def_function.py:126] 5 out of the last 6023 calls to <function TFStepMetric._update_state at 0x7f363255cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From train_eval_clip_agent.py:347: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.
Instructions for updating:
Use `as_dataset(..., single_deterministic_pass=True)` instead.
W0517 08:49:26.889573 139875764696896 deprecation.py:339] From train_eval_clip_agent.py:347: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.
Instructions for updating:
Use `as_dataset(..., single_deterministic_pass=True)` instead.
I0517 08:50:47.629021 139875764696896 train_eval_clip_agent.py:368] step = 0, loss = -0.014573
I0517 08:50:47.629784 139875764696896 train_eval_clip_agent.py:370] 0.000 steps/sec
I0517 08:50:47.629861 139875764696896 train_eval_clip_agent.py:372] collect_time = 1384.379, train_time = 80.740
I0517 08:56:16.879312 139875764696896 metric_utils.py:47]  
		 AverageReturn = -0.21828427910804749
		 AverageEpisodeLength = 300.0
I0517 08:56:17.573053 139875764696896 common.py:1003] Saved checkpoint: train_output/train/ckpt-0
I0517 08:56:17.880429 139875764696896 common.py:1003] Saved checkpoint: train_output/train/policy/ckpt-0
I0517 09:42:57.990433 139875764696896 train_eval_clip_agent.py:368] step = 50, loss = -0.031159
I0517 09:42:57.991083 139875764696896 train_eval_clip_agent.py:370] 0.018 steps/sec
I0517 09:42:57.991158 139875764696896 train_eval_clip_agent.py:372] collect_time = 1390.500, train_time = 8.003
I0517 10:29:32.283569 139875764696896 train_eval_clip_agent.py:368] step = 100, loss = -0.026040
I0517 10:29:32.290354 139875764696896 train_eval_clip_agent.py:370] 0.018 steps/sec
I0517 10:29:32.290432 139875764696896 train_eval_clip_agent.py:372] collect_time = 1388.494, train_time = 8.013
I0517 11:15:53.283087 139875764696896 train_eval_clip_agent.py:368] step = 150, loss = 0.260817
I0517 11:15:53.290769 139875764696896 train_eval_clip_agent.py:370] 0.018 steps/sec
I0517 11:15:53.290833 139875764696896 train_eval_clip_agent.py:372] collect_time = 1376.600, train_time = 8.363
2021-05-17 12:02:26.558467: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
I0517 12:02:33.946729 139875764696896 train_eval_clip_agent.py:368] step = 200, loss = 1.091156
I0517 12:02:33.959263 139875764696896 train_eval_clip_agent.py:370] 0.017 steps/sec
I0517 12:02:33.959555 139875764696896 train_eval_clip_agent.py:372] collect_time = 1347.410, train_time = 86.728
I0517 12:46:22.626076 139875764696896 train_eval_clip_agent.py:368] step = 250, loss = 1.200150
I0517 12:46:22.815802 139875764696896 train_eval_clip_agent.py:370] 0.019 steps/sec
I0517 12:46:22.816103 139875764696896 train_eval_clip_agent.py:372] collect_time = 1216.129, train_time = 72.315
2021-05-17 13:29:49.953122: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
I0517 13:29:56.730352 139875764696896 train_eval_clip_agent.py:368] step = 300, loss = 0.907508
I0517 13:29:56.731734 139875764696896 train_eval_clip_agent.py:370] 0.018 steps/sec
I0517 13:29:56.731885 139875764696896 train_eval_clip_agent.py:372] collect_time = 1269.695, train_time = 83.749
I0517 14:11:07.254714 139875764696896 train_eval_clip_agent.py:368] step = 350, loss = 1.781046
I0517 14:11:07.264846 139875764696896 train_eval_clip_agent.py:370] 0.022 steps/sec
I0517 14:11:07.265136 139875764696896 train_eval_clip_agent.py:372] collect_time = 1057.054, train_time = 95.944
2021-05-17 14:36:01.321299: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 2.62G (2818247168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-05-17 14:36:02.422292: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
2021-05-17 14:36:02.512219: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 4.66G (4999285248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-05-17 14:36:02.512342: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
I0517 14:52:32.209818 139875764696896 train_eval_clip_agent.py:368] step = 400, loss = 1.233341
I0517 14:52:32.489910 139875764696896 train_eval_clip_agent.py:370] 0.026 steps/sec
I0517 14:52:32.490223 139875764696896 train_eval_clip_agent.py:372] collect_time = 908.735, train_time = 71.151
I0517 15:24:57.497624 139875764696896 train_eval_clip_agent.py:368] step = 450, loss = 1.605551
I0517 15:24:57.499668 139875764696896 train_eval_clip_agent.py:370] 0.024 steps/sec
I0517 15:24:57.499950 139875764696896 train_eval_clip_agent.py:372] collect_time = 954.737, train_time = 92.305
I0517 16:00:00.389630 139875764696896 train_eval_clip_agent.py:368] step = 500, loss = 0.973360
I0517 16:00:00.390968 139875764696896 train_eval_clip_agent.py:370] 0.022 steps/sec
I0517 16:00:00.391086 139875764696896 train_eval_clip_agent.py:372] collect_time = 1024.731, train_time = 102.367
I0517 16:04:23.546968 139875764696896 metric_utils.py:47]  
		 AverageReturn = 5.344974517822266
		 AverageEpisodeLength = 190.8000030517578
I0517 16:04:28.336416 139875764696896 common.py:1003] Saved checkpoint: train_output/train/ckpt-500
I0517 16:04:28.671295 139875764696896 common.py:1003] Saved checkpoint: train_output/train/policy/ckpt-500
I0517 16:39:47.746194 139875764696896 train_eval_clip_agent.py:368] step = 550, loss = 0.895794
I0517 16:39:48.095992 139875764696896 train_eval_clip_agent.py:370] 0.027 steps/sec
I0517 16:39:48.096324 139875764696896 train_eval_clip_agent.py:372] collect_time = 841.638, train_time = 95.937
